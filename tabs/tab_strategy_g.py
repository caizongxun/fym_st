"""
Strategy G v1.2 - Deep Q-Learning with Enhanced State & Shaped Reward

æ ¸å¿ƒç†å¿µ:
ä¸é æ¸¬æ–¹å‘ï¼Œç›´æ¥å­¸ç¿’ã€Œè³ºéŒ¢çš„è¡Œç‚ºã€

v1.2 é©å‘½æ€§æ”¹é€²:
1. ç‹€æ…‹ç©ºé–“æ“´å……: 10ç¶­ â†’ 17ç¶­
   - æ–°å¢å¸‚å ´ç‹€æ…‹åˆ¤æ–· (è¶¨å‹¢/éœ‡ç›ª/æ³¢å‹•)
   - æ–°å¢ Agent è‡ªæˆ‘èªçŸ¥ (å‹ç‡/é€£è™§/å›æ’¤)
2. åˆ†éšæ®µ Reward: æŒå€‰éç¨‹ä¹Ÿçµ¦åé¥‹
   - æ–¹å‘å°äº† â†’ å°çå‹µ
   - æ­¢ææ‹–å»¶ â†’ æŒçºŒæ‡²ç½°
   - æµ®ç›ˆä¸è·‘ â†’ è²ªå©ªæ‡²ç½°
   - é€£è™§é–‹å€‰ â†’ é¢¨æ§æ‡²ç½°
3. 1h/1dé€±æœŸ: ç›¸å®¹ HuggingFace è³‡æ–™é›†
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from collections import deque
import random
import plotly.graph_objects as go

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    st.warning("PyTorch æœªå®‰è£ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆ Q-Learning")

from data.binance_loader import BinanceDataLoader


class TradingEnvV2:
    """
    v1.2 å¢å¼·äº¤æ˜“ç’°å¢ƒ
    """
    def __init__(self, df, capital=10000.0, leverage=3, fee_rate=0.0006, position_size=0.3):
        self.df = df.reset_index(drop=True)
        self.initial_capital = capital
        self.leverage = leverage
        self.fee_rate = fee_rate
        self.position_size = position_size
        
        self._calculate_features()
        
        self.state_dim = 17  # v1.2: æ“´å……åˆ° 17 ç¶­
        self.action_dim = 4
        
        # v1.2: Agent è¨˜æ†¶
        self.trade_history = deque(maxlen=10)  # æœ€è¿‘ 10 ç­†äº¤æ˜“
        self.peak_capital = capital
        
        self.reset()
    
    def _calculate_features(self):
        df = self.df
        # ATR
        df['tr'] = np.maximum(
            df['high'] - df['low'],
            np.maximum(abs(df['high'] - df['close'].shift(1)),
                       abs(df['low'] - df['close'].shift(1)))
        )
        df['atr'] = df['tr'].rolling(14).mean()
        
        # EMA
        df['ema20'] = df['close'].ewm(span=20).mean()
        df['ema50'] = df['close'].ewm(span=50).mean()
        df['ema_dist'] = (df['close'] - df['ema20']) / (df['atr'] + 1e-8)
        
        # RSI
        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        df['rsi'] = 100 - (100 / (1 + gain / (loss + 1e-8)))
        
        # MACD
        ema12 = df['close'].ewm(span=12).mean()
        ema26 = df['close'].ewm(span=26).mean()
        df['macd'] = ema12 - ema26
        df['macd_signal'] = df['macd'].ewm(span=9).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']
        
        # Volume
        df['volume_ma'] = df['volume'].rolling(20).mean()
        df['volume_ratio'] = df['volume'] / (df['volume_ma'] + 1e-8)
        
        # v1.2: æ–°å¢ç‰¹å¾µ
        # ADX (è¶¨å‹¢å¼·åº¦)
        plus_dm = df['high'].diff()
        minus_dm = -df['low'].diff()
        plus_dm = plus_dm.where((plus_dm > minus_dm) & (plus_dm > 0), 0)
        minus_dm = minus_dm.where((minus_dm > plus_dm) & (minus_dm > 0), 0)
        tr_smooth = df['tr'].rolling(14).mean()
        plus_di = 100 * (plus_dm.rolling(14).mean() / (tr_smooth + 1e-8))
        minus_di = 100 * (minus_dm.rolling(14).mean() / (tr_smooth + 1e-8))
        dx = 100 * np.abs(plus_di - minus_di) / (plus_di + minus_di + 1e-8)
        df['adx'] = dx.rolling(14).mean()
        
        # BB
        df['bb_mid'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        df['bb_upper'] = df['bb_mid'] + 2 * bb_std
        df['bb_lower'] = df['bb_mid'] - 2 * bb_std
        df['bb_pct'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'] + 1e-8)
        
        # ATR ç™¾åˆ†ä½ (æ³¢å‹•ç‹€æ…‹)
        df['atr_pct'] = df['atr'] / df['close']
        df['atr_percentile'] = df['atr_pct'].rolling(50).apply(lambda x: (x.iloc[-1] > x).sum() / len(x) if len(x) > 0 else 0.5)
        
        df.fillna(0, inplace=True)
        self.df = df
    
    def reset(self, start_idx=60):
        self.current_step = start_idx
        self.capital = self.initial_capital
        self.peak_capital = self.initial_capital
        self.position = 0
        self.entry_price = 0
        self.hold_time = 0
        self.total_trades = 0
        self.winning_trades = 0
        self.trade_history.clear()
        self.consecutive_losses = 0
        
        return self._get_state()
    
    def _get_state(self):
        row = self.df.iloc[self.current_step]
        
        # åŸºç¤å€‰ä½ç‹€æ…‹ (3)
        position_encoded = self.position
        hold_time_norm = min(self.hold_time / 30.0, 1.0)
        
        if self.position != 0:
            pnl_ratio = (row['close'] - self.entry_price) / self.entry_price * self.position * 100
            pnl_ratio = np.clip(pnl_ratio / 10.0, -1.0, 1.0)
        else:
            pnl_ratio = 0
        
        # å¸‚å ´ç‰¹å¾µ (7)
        rsi_norm = row['rsi'] / 100.0
        macd_hist_norm = np.clip(row['macd_hist'] / (row['atr'] + 1e-8), -2, 2) / 2.0
        ema_dist_norm = np.clip(row['ema_dist'], -3, 3) / 3.0
        volume_ratio_norm = np.clip(row['volume_ratio'], 0, 3) / 3.0
        bb_pct_norm = np.clip(row['bb_pct'], 0, 1)
        
        roc_5 = (row['close'] - self.df.iloc[max(0, self.current_step - 5)]['close']) / (self.df.iloc[max(0, self.current_step - 5)]['close'] + 1e-8) * 100
        roc_5_norm = np.clip(roc_5 / 5.0, -1, 1)
        
        # v1.2: æ–°å¢å¸‚å ´ç‹€æ…‹ (3)
        trend_strength = np.clip(row['adx'] / 50.0, 0, 1)  # ADX æ­£è¦åŒ–
        price_vs_ma20 = 1 if row['close'] > row['ema20'] else -1
        volatility_regime = np.clip(row['atr_percentile'], 0, 1)
        
        # v1.2: Agent è‡ªæˆ‘èªçŸ¥ (4)
        recent_win_rate = 0.5
        if len(self.trade_history) >= 3:
            wins = sum(1 for t in self.trade_history if t > 0)
            recent_win_rate = wins / len(self.trade_history)
        
        consecutive_losses_norm = min(self.consecutive_losses / 5.0, 1.0)
        
        capital_usage = 0
        if self.position != 0:
            capital_usage = self.position_size  # å·²ç”¨å€‰ä½æ¯”ä¾‹
        
        max_dd = 0
        if self.peak_capital > 0:
            max_dd = max(0, (self.peak_capital - self.capital) / self.peak_capital)
        max_dd_norm = min(max_dd, 1.0)
        
        state = np.array([
            # å€‰ä½ç‹€æ…‹ (3)
            position_encoded,
            hold_time_norm,
            pnl_ratio,
            # å¸‚å ´ç‰¹å¾µ (7)
            rsi_norm,
            macd_hist_norm,
            ema_dist_norm,
            volume_ratio_norm,
            bb_pct_norm,
            roc_5_norm,
            0,  # ä¿ç•™ä½
            # å¸‚å ´ç‹€æ…‹ (3)
            trend_strength,
            price_vs_ma20,
            volatility_regime,
            # Agent èªçŸ¥ (4)
            recent_win_rate,
            consecutive_losses_norm,
            capital_usage,
            max_dd_norm,
        ], dtype=np.float32)
        
        return state
    
    def step(self, action):
        """
        v1.2: åˆ†éšæ®µ Reward
        """
        row = self.df.iloc[self.current_step]
        reward = 0
        done = False
        info = {}
        
        # Action 0: é–‹å¤šå€‰
        if action == 0 and self.position == 0:
            self.position = 1
            self.entry_price = row['close']
            self.hold_time = 0
            reward = -0.01
            
            # v1.2: é€£è™§å¾Œé–‹å€‰é‡ç½°
            if self.consecutive_losses >= 3:
                reward -= 1.0
        
        # Action 1: é–‹ç©ºå€‰
        elif action == 1 and self.position == 0:
            self.position = -1
            self.entry_price = row['close']
            self.hold_time = 0
            reward = -0.01
            
            if self.consecutive_losses >= 3:
                reward -= 1.0
        
        # Action 2: å¹³å€‰
        elif action == 2 and self.position != 0:
            exit_price = row['close']
            pnl_pct = (exit_price - self.entry_price) / self.entry_price * self.position * 100
            
            fee = self.fee_rate * 2 * 100
            leveraged_pnl = pnl_pct * self.leverage - fee
            actual_pnl = self.capital * self.position_size * leveraged_pnl / 100
            
            self.capital += actual_pnl
            if self.capital > self.peak_capital:
                self.peak_capital = self.capital
            
            # v1.2: æ”¹é€² Reward
            if leveraged_pnl > 0:
                # ç²åˆ©åŠ æ¬Š
                base_reward = leveraged_pnl / 10.0 * 1.5
                # çå‹µç©©å®šå°è´ (1-3%)
                if 1.0 < leveraged_pnl < 3.0:
                    base_reward += 1.0
                # æ‡²ç½°éåº¦æ³¢å‹• (>5%)
                elif leveraged_pnl > 5.0:
                    base_reward -= (leveraged_pnl - 5.0) * 0.2
            else:
                # è™§æé‡ç½° (å­¸ç¿’å¿«é€Ÿæ­¢æ)
                base_reward = leveraged_pnl / 10.0 * 2.5
                # ç‰¹åˆ¥æ‡²ç½°å¤§è™§ (>3%)
                if leveraged_pnl < -3.0:
                    base_reward -= abs(leveraged_pnl) * 0.3
            
            reward = base_reward
            
            self.total_trades += 1
            self.trade_history.append(actual_pnl)
            
            if actual_pnl > 0:
                self.winning_trades += 1
                self.consecutive_losses = 0
            else:
                self.consecutive_losses += 1
            
            info = {
                'trade': True,
                'pnl': actual_pnl,
                'pnl_pct': leveraged_pnl,
                'hold_time': self.hold_time
            }
            
            self.position = 0
            self.hold_time = 0
        
        # Action 3: æŒæœ‰
        else:
            if self.position != 0:
                self.hold_time += 1
                
                # v1.2: åˆ†éšæ®µåé¥‹
                unrealized_pnl = (row['close'] - self.entry_price) / self.entry_price * self.position * 100 * self.leverage
                
                # 1. æ–¹å‘å°äº† â†’ å°çå‹µ
                if unrealized_pnl > 0.5:
                    reward += 0.05
                
                # 2. æ­¢ææ‹–å»¶ â†’ æŒçºŒæ‡²ç½°
                if unrealized_pnl < -2.0:
                    reward -= 0.5 * min(self.hold_time / 5.0, 2.0)  # æ‹–è¶Šä¹…ç½°è¶Šé‡
                
                # 3. æµ®ç›ˆä¸è·‘ â†’ è²ªå©ªæ‡²ç½°
                if unrealized_pnl > 4.0 and self.hold_time > 15:
                    reward -= 0.3
                
                # 4. åŸºç¤æŒå€‰æˆæœ¬
                reward -= 0.002 * min(self.hold_time / 10.0, 1.0)
            else:
                reward = 0
        
        # æª¢æŸ¥çµæŸ
        self.current_step += 1
        if self.current_step >= len(self.df) - 1:
            done = True
            if self.position != 0:
                row_final = self.df.iloc[self.current_step]
                pnl_pct = (row_final['close'] - self.entry_price) / self.entry_price * self.position * 100
                fee = self.fee_rate * 2 * 100
                leveraged_pnl = pnl_pct * self.leverage - fee
                actual_pnl = self.capital * self.position_size * leveraged_pnl / 100
                self.capital += actual_pnl
                reward += leveraged_pnl / 10.0
        
        # çˆ†å€‰æª¢æŸ¥
        if self.capital < self.initial_capital * 0.5:
            done = True
            reward = -10
        
        next_state = self._get_state() if not done else np.zeros(self.state_dim)
        
        return next_state, reward, done, info


class DQNAgentV2:
    """
    v1.2: æ“´å……ç¶²è·¯ä»¥é©æ‡‰ 17 ç¶­ç‹€æ…‹
    """
    def __init__(self, state_dim, action_dim, lr=0.0001):
        if not TORCH_AVAILABLE:
            raise ImportError("éœ€è¦å®‰è£ PyTorch")
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.memory = deque(maxlen=10000)
        self.gamma = 0.90
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.99
        self.batch_size = 64
        
        # v1.2: åŠ æ·±ç¶²è·¯ (17 â†’ 128 â†’ 64 â†’ 4)
        self.model = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, action_dim)
        )
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)
        self.criterion = nn.MSELoss()
    
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state, training=True):
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_dim)
        
        if training:
            self.model.train()
        else:
            self.model.eval()
        
        with torch.no_grad():
            state_t = torch.FloatTensor(state).unsqueeze(0)
            q_values = self.model(state_t)
            return q_values.argmax().item()
    
    def replay(self):
        if len(self.memory) < self.batch_size:
            return 0
        
        minibatch = random.sample(self.memory, self.batch_size)
        
        states = torch.FloatTensor([x[0] for x in minibatch])
        actions = torch.LongTensor([x[1] for x in minibatch])
        rewards = torch.FloatTensor([x[2] for x in minibatch])
        next_states = torch.FloatTensor([x[3] for x in minibatch])
        dones = torch.FloatTensor([x[4] for x in minibatch])
        
        self.model.train()
        current_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        
        self.model.eval()
        with torch.no_grad():
            next_q = self.model(next_states).max(1)[0]
        target_q = rewards + (1 - dones) * self.gamma * next_q
        
        self.model.train()
        loss = self.criterion(current_q, target_q.detach())
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        return loss.item()


def train_agent(env, agent, episodes=100):
    episode_rewards = []
    episode_capitals = []
    
    for e in range(episodes):
        state = env.reset()
        done = False
        total_reward = 0
        
        while not done:
            action = agent.act(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            
            if isinstance(agent, DQNAgentV2):
                agent.replay()
            
            state = next_state
            total_reward += reward
        
        episode_rewards.append(total_reward)
        episode_capitals.append(env.capital)
        
        if (e + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_capital = np.mean(episode_capitals[-10:])
            st.text(f"Episode {e+1}/{episodes} | Avg Reward: {avg_reward:.2f} | Avg Capital: ${avg_capital:.0f}")
    
    return episode_rewards, episode_capitals


def backtest_agent(env, agent):
    state = env.reset()
    done = False
    trades = []
    equity_curve = [env.capital]
    
    while not done:
        action = agent.act(state, training=False)
        next_state, reward, done, info = env.step(action)
        
        if info.get('trade'):
            trades.append({
                'step': env.current_step,
                'pnl': info['pnl'],
                'hold_time': info['hold_time']
            })
        
        equity_curve.append(env.capital)
        state = next_state
    
    return trades, equity_curve, env


def render_strategy_g_tab(loader, symbol_selector):
    st.header("ç­–ç•¥ G: å¼·åŒ–å­¸ç¿’ Agent v1.2 ğŸš€")

    with st.expander("âš¡ v1.2 é©å‘½æ€§å‡ç´š", expanded=True):
        st.markdown("""
        **v1.1 å•é¡Œ**: ç›ˆè™§æ¯”å¤ªå·® (0.72)ï¼Œå¹³å‡è™§æ > å¹³å‡ç²åˆ©
        
        **v1.2 æ ¸å¿ƒå‰µæ–°**:
        
        1ï¸âƒ£ **ç‹€æ…‹ç©ºé–“æ“´å……**: 10ç¶­ â†’ 17ç¶­
        - å¸‚å ´ç‹€æ…‹: è¶¨å‹¢å¼·åº¦(ADX)ã€åƒ¹æ ¼ä½ç½®ã€æ³¢å‹•ç‹€æ…‹
        - Agent è‡ªæˆ‘èªçŸ¥: è¿‘æœŸå‹ç‡ã€é€£è™§æ¬¡æ•¸ã€è³‡é‡‘ä½¿ç”¨ã€æœ€å¤§å›æ’¤
        
        2ï¸âƒ£ **åˆ†éšæ®µ Reward**: æŒå€‰éç¨‹ä¹Ÿçµ¦åé¥‹
        - âœ… æ–¹å‘å°äº† â†’ å°çå‹µ (+0.05)
        - âŒ æ­¢ææ‹–å»¶ â†’ æŒçºŒæ‡²ç½° (-0.5 * æ™‚é–“)
        - âŒ æµ®ç›ˆä¸è·‘ â†’ è²ªå©ªæ‡²ç½° (-0.3)
        - âŒ é€£è™§é–‹å€‰ â†’ é¢¨æ§æ‡²ç½° (-1.0)
        
        3ï¸âƒ£ **å°ç¨±ç›ˆè™§æ¯” Reward**:
        - ç²åˆ©åŠ æ¬Š 1.5x
        - è™§æé‡ç½° 2.5x
        - å¼·åˆ¶å­¸ç¿’ã€Œå¤§è´å°è¼¸ã€
        
        ğŸ’¡ **å»ºè­°**: HuggingFace ç”¨ 1hï¼ŒBinance API å¯ç”¨ 4h
        """)

    st.markdown("---")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**æ•¸æ“š**")
        symbol_list = symbol_selector("strategy_g", multi=False)
        symbol = symbol_list[0]
        train_days = st.slider("è¨“ç·´å¤©æ•¸", 60, 240, 120, key="train_g")
        test_days = st.slider("æ¸¬è©¦å¤©æ•¸", 14, 60, 30, key="test_g")
        
        # v1.2: æ ¹æ“šè³‡æ–™æºèª¿æ•´é¸é …
        if isinstance(loader, BinanceDataLoader):
            timeframe = st.selectbox("æ™‚é–“å‘¨æœŸ", ['1h', '4h'], index=1, key="tf_g")
        else:
            timeframe = st.selectbox("æ™‚é–“å‘¨æœŸ", ['15m', '1h', '1d'], index=1, key="tf_g")
            st.caption("ğŸ’¡ HuggingFace ä¸æ”¯æ´ 4hï¼Œåˆ‡æ›åˆ° Binance API å¯ç”¨")
        
        bars_per_day = {'15m': 96, '1h': 24, '4h': 6, '1d': 1}.get(timeframe, 24)

    with col2:
        st.markdown("**RL åƒæ•¸**")
        episodes = st.slider("è¨“ç·´è¼ªæ•¸", 50, 200, 100, 10, key="ep_g")
        learning_rate = st.select_slider("å­¸ç¿’ç‡", [0.00005, 0.0001, 0.0005, 0.001], value=0.0001, key="lr_g")
        capital = st.number_input("è³‡é‡‘", 1000.0, 100000.0, 10000.0, 1000.0, key="cap_g")
        leverage = st.slider("æ§“æ¡¿", 1, 10, 3, key="lev_g")
        position_size = st.slider("å€‰ä½%", 10, 80, 30, 5, key="pos_g") / 100.0
        
        st.success("âœ¨ v1.2: 17ç¶­ç‹€æ…‹ + åˆ†éšæ®µReward")

    if st.button("ğŸš€ è¨“ç·´ v1.2 Agent", type="primary", use_container_width=True):
        prog = st.progress(0)
        stat = st.empty()
        
        try:
            stat.text("è¼‰å…¥æ•¸æ“š...")
            prog.progress(10)
            total_days = train_days + test_days + 5
            
            if isinstance(loader, BinanceDataLoader):
                end = datetime.now()
                df_all = loader.load_historical_data(symbol, timeframe, end - timedelta(days=total_days), end)
            else:
                df_all = loader.load_klines(symbol, timeframe)
                df_all = df_all.tail(total_days * bars_per_day)
            
            df_all = df_all.reset_index(drop=True)
            split_idx = int(len(df_all) * (train_days / (train_days + test_days)))
            df_train = df_all.iloc[:split_idx].reset_index(drop=True)
            df_test = df_all.iloc[split_idx:].reset_index(drop=True)
            
            st.info(f"è¨“ç·´: {len(df_train)} æ ¹ | æ¸¬è©¦: {len(df_test)} æ ¹")
            prog.progress(20)
            
            stat.text("åˆå§‹åŒ– v1.2 ç’°å¢ƒ...")
            train_env = TradingEnvV2(df_train, capital, leverage, position_size=position_size)
            test_env = TradingEnvV2(df_test, capital, leverage, position_size=position_size)
            prog.progress(25)
            
            stat.text("å‰µå»º DQN v1.2 Agent...")
            if TORCH_AVAILABLE:
                agent = DQNAgentV2(train_env.state_dim, train_env.action_dim, lr=learning_rate)
            else:
                st.error("v1.2 éœ€è¦ PyTorchï¼Œè«‹å®‰è£: pip install torch")
                return
            prog.progress(30)
            
            stat.text(f"è¨“ç·´ä¸­ ({episodes} è¼ª)...")
            episode_rewards, episode_capitals = train_agent(train_env, agent, episodes)
            prog.progress(70)
            
            st.markdown("### è¨“ç·´éç¨‹")
            fig_train = go.Figure()
            fig_train.add_trace(go.Scatter(y=episode_capitals, mode='lines', name='æ¬Šç›Š'))
            fig_train.add_hline(y=capital, line_dash="dash", line_color="gray", annotation_text="åˆå§‹è³‡é‡‘")
            fig_train.update_layout(title="è¨“ç·´è¼ªæ¬Šç›Šè®ŠåŒ–", xaxis_title="Episode", yaxis_title="Capital ($)")
            st.plotly_chart(fig_train, use_container_width=True)
            
            c1, c2, c3 = st.columns(3)
            c1.metric("æœ€çµ‚è¨“ç·´æ¬Šç›Š", f"${episode_capitals[-1]:,.0f}")
            c2.metric("å¹³å‡ Reward", f"{np.mean(episode_rewards[-10:]):.2f}")
            train_return = (episode_capitals[-1] - capital) / capital * 100
            c3.metric("è¨“ç·´å ±é…¬", f"{train_return:.1f}%")
            
            stat.text("å›æ¸¬...")
            prog.progress(80)
            trades, equity_curve, final_env = backtest_agent(test_env, agent)
            prog.progress(100)
            stat.text("å®Œæˆ")
            
            st.markdown("### å›æ¸¬çµæœ")
            final_capital = equity_curve[-1]
            total_return = (final_capital - capital) / capital * 100
            
            c1, c2, c3 = st.columns(3)
            c1.metric("æœ€çµ‚æ¬Šç›Š", f"${final_capital:,.0f}", f"{final_capital - capital:+,.0f}")
            c2.metric("ç¸½å ±é…¬", f"{total_return:.1f}%")
            c3.metric("äº¤æ˜“æ¬¡æ•¸", len(trades))
            
            if len(trades) > 0:
                wins = [t for t in trades if t['pnl'] > 0]
                losses = [t for t in trades if t['pnl'] <= 0]
                win_rate = len(wins) / len(trades) * 100
                avg_win = np.mean([t['pnl'] for t in wins]) if wins else 0
                avg_loss = np.mean([t['pnl'] for t in losses]) if losses else 0
                profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else 0
                
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("å‹ç‡", f"{win_rate:.1f}%")
                c2.metric("å¹³å‡ç²åˆ©", f"${avg_win:.2f}")
                c3.metric("å¹³å‡è™§æ", f"${avg_loss:.2f}")
                c4.metric("ç›ˆè™§æ¯”", f"{profit_factor:.2f}")
            
            fig_equity = go.Figure()
            fig_equity.add_trace(go.Scatter(y=equity_curve, mode='lines', name='æ¬Šç›Š', line=dict(color='blue')))
            fig_equity.add_hline(y=capital, line_dash="dash", line_color="gray", annotation_text="åˆå§‹è³‡é‡‘")
            fig_equity.update_layout(title="æ¬Šç›Šæ›²ç·š", xaxis_title="Steps", yaxis_title="Capital ($)")
            st.plotly_chart(fig_equity, use_container_width=True)
            
            if trades:
                st.subheader("äº¤æ˜“è¨˜éŒ„")
                trades_df = pd.DataFrame(trades)
                st.dataframe(trades_df.tail(20), use_container_width=True)
                
                # éæ“¬åˆæª¢æŸ¥
                overfitting_ratio = episode_capitals[-1] / max(final_capital, 1)
                if overfitting_ratio > 5:
                    st.warning(f"âš ï¸ éæ“¬åˆé¢¨éšª: {overfitting_ratio:.1f}x")
                elif overfitting_ratio > 2:
                    st.info(f"â„¹ï¸ è¼•å¾®éæ“¬åˆ: {overfitting_ratio:.1f}x")
                else:
                    st.success(f"âœ… æ³›åŒ–è‰¯å¥½: {overfitting_ratio:.1f}x")
                    
                # v1.2: ç›ˆè™§æ¯”æª¢æŸ¥
                if len(trades) > 10:
                    if profit_factor > 1.2:
                        st.success(f"âœ… ç›ˆè™§æ¯”å„ªç§€: {profit_factor:.2f} (ç›®æ¨™ >1.2)")
                    elif profit_factor > 0.8:
                        st.info(f"â„¹ï¸ ç›ˆè™§æ¯”å¯æ¥å—: {profit_factor:.2f}")
                    else:
                        st.warning(f"âš ï¸ ç›ˆè™§æ¯”åä½: {profit_factor:.2f} (éœ€æ”¹é€²)")
        
        except Exception as e:
            st.error(f"éŒ¯èª¤: {e}")
            import traceback
            with st.expander("è©³æƒ…"): st.code(traceback.format_exc())
