"""
Strategy K - RL Agent æ¿€é€²æ”¹é€ ç‰ˆ
Aggressive RL Agent Remake

ç›®æ¨™: 30å¤© +100-150% (æœ€é«˜æ½›åŠ›)

æ”¹é€²é»:
1. æ§“æ¡¿: 5x â†’ 10x
2. Reward å‡½æ•¸: æ”¹ç‚ºã€Œæ—¥å ±é…¬ç‡ã€
3. å…è¨±å¤šå–®é‡ç–Š (é‡‘å­—å¡”åŠ å€‰)
4. æœ€å¤§å€‰ä½: 200% (10x * 2å€‰)
5. è¨“ç·´ç›®æ¨™: ä¸æ˜¯ã€Œè³ºéŒ¢ã€è€Œæ˜¯ã€Œå¿«é€Ÿè³ºéŒ¢ã€

é¢¨éšª:
- å¯èƒ½éæ“¬åˆ
- å¯èƒ½çˆ†å€‰ (-100%)
- ä¸å¯é æ¸¬ (é»‘ç›’)
- æœ€é«˜æ½›åŠ› (+150%+)
"""

import streamlit as st
import pandas as pd
import numpy as np
import gymnasium as gym
from gymnasium import spaces
import plotly.graph_objects as go

try:
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import DummyVecEnv
    SB3_AVAILABLE = True
except ImportError:
    SB3_AVAILABLE = False

from strategies.multi_timeframe import MultiTimeframeLoader


class AggressiveTradingEnv(gym.Env):
    """
    æ¿€é€²äº¤æ˜“ç’°å¢ƒ - é‡‘å­—å¡”åŠ å€‰
    """
    
    def __init__(self, df: pd.DataFrame, initial_balance: float = 10000, leverage: int = 10):
        super().__init__()
        
        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance
        self.leverage = leverage
        self.max_positions = 2  # å…è¨± 2 å€‹å€‰ä½é‡ç–Š
        
        # Action: [hold, long1, long2, short1, short2, close_all]
        self.action_space = spaces.Discrete(6)
        
        # Observation: [price, indicators, positions]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(15,), dtype=np.float32
        )
        
        self.reset()
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        self.current_step = 50  # éœ€è¦æŒ‡æ¨™
        self.balance = self.initial_balance
        self.positions = []  # [{direction, entry_price, size}]
        self.trades = []
        self.initial_step_balance = self.balance
        
        return self._get_observation(), {}
    
    def _get_observation(self):
        row = self.df.iloc[self.current_step]
        
        # è¨ˆç®—æŒ‡æ¨™
        close_prices = self.df['close'].iloc[self.current_step-50:self.current_step+1]
        ema8 = close_prices.ewm(span=8).mean().iloc[-1]
        ema20 = close_prices.ewm(span=20).mean().iloc[-1]
        ema50 = close_prices.ewm(span=50).mean().iloc[-1]
        
        rsi_period = 14
        delta = close_prices.diff()
        gain = delta.where(delta > 0, 0).rolling(rsi_period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(rsi_period).mean()
        rs = gain / (loss + 1e-8)
        rsi = (100 - (100 / (1 + rs))).iloc[-1]
        
        # æŒå€‰ç‹€æ…‹
        total_position_size = sum(p['size'] for p in self.positions)
        net_position = sum(p['direction'] * p['size'] for p in self.positions)  # æ­£=å¤š, è² =ç©º
        
        obs = np.array([
            row['close'] / 100000,  # æ­£è¦åŒ–åƒ¹æ ¼
            (row['close'] - ema8) / row['close'],
            (row['close'] - ema20) / row['close'],
            (row['close'] - ema50) / row['close'],
            rsi / 100,
            row['volume'] / row['volume'],  # æ­£è¦åŒ–é‡
            self.balance / self.initial_balance,
            total_position_size,
            net_position,
            len(self.positions) / self.max_positions,
            (row['high'] - row['low']) / row['close'],  # ATR proxy
            row['close'] / row['open'] - 1,  # ç•¶æ ¹Kæ£’æ¼²è·Œ
            (close_prices.iloc[-1] - close_prices.iloc[-5]) / close_prices.iloc[-5],  # 5æ ¹å‹•é‡
            (close_prices.iloc[-1] - close_prices.iloc[-10]) / close_prices.iloc[-10],  # 10æ ¹å‹•é‡
            (close_prices.iloc[-1] - close_prices.iloc[-20]) / close_prices.iloc[-20]   # 20æ ¹å‹•é‡
        ], dtype=np.float32)
        
        return obs
    
    def step(self, action):
        current_price = self.df.iloc[self.current_step]['close']
        fee = 0.0006
        position_size = 0.5  # æ¯å€‹å€‰ä½ 50%
        
        reward = 0
        
        # Action: 0=hold, 1=long1, 2=long2, 3=short1, 4=short2, 5=close_all
        if action == 1 and len(self.positions) < self.max_positions:  # Long 1å€‹å€‰
            self.positions.append({
                'direction': 1,
                'entry_price': current_price,
                'size': position_size
            })
        
        elif action == 2 and len(self.positions) < self.max_positions:  # Long åŠ å€‰
            self.positions.append({
                'direction': 1,
                'entry_price': current_price,
                'size': position_size
            })
        
        elif action == 3 and len(self.positions) < self.max_positions:  # Short 1å€‹å€‰
            self.positions.append({
                'direction': -1,
                'entry_price': current_price,
                'size': position_size
            })
        
        elif action == 4 and len(self.positions) < self.max_positions:  # Short åŠ å€‰
            self.positions.append({
                'direction': -1,
                'entry_price': current_price,
                'size': position_size
            })
        
        elif action == 5 and len(self.positions) > 0:  # å¹³æ‰€æœ‰å€‰
            for pos in self.positions:
                pnl_pct = (current_price - pos['entry_price']) / pos['entry_price'] * pos['direction']
                pnl = self.initial_balance * pos['size'] * (pnl_pct * self.leverage - fee * 2)
                self.balance += pnl
                reward += pnl
                
                self.trades.append({
                    'entry_price': pos['entry_price'],
                    'exit_price': current_price,
                    'direction': pos['direction'],
                    'pnl': pnl
                })
            
            self.positions = []
        
        # ç§»å‹•åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1
        done = self.current_step >= len(self.df) - 1
        
        # Reward è¨­è¨ˆ: æ—¥å ±é…¬ç‡ (é¼“å‹µå¿«é€Ÿè³ºéŒ¢)
        step_return = (self.balance - self.initial_step_balance) / self.initial_step_balance * 100
        reward = step_return * 10  # æ”¾å¤§ reward
        self.initial_step_balance = self.balance
        
        # çˆ†å€‰æ‡²ç½°
        if self.balance < self.initial_balance * 0.5:
            reward = -1000
            done = True
        
        return self._get_observation(), reward, done, False, {}
    
    def render(self):
        pass


def render_strategy_k_tab(loader, symbol_selector):
    st.header("ç­–ç•¥ K: RL Agent æ¿€é€²ç‰ˆ ğŸ¤–ğŸ”¥")

    with st.expander("âš ï¸ æ¥µé«˜é¢¨éšªè­¦å‘Š", expanded=True):
        st.markdown("""
        **ç›®æ¨™**: 30å¤© +100-150% (æœ€é«˜æ½›åŠ›)
        
        ğŸ¤– **RL Agent æ”¹é€ **:
        - 10x æ§“æ¡¿ (æ”¾å¤§2å€)
        - å…è¨±å¤šå€‰é‡ç–Š (é‡‘å­—å¡”åŠ å€‰)
        - æœ€å¤§å€‰ä½ 200% (2å€‹ 100%å€‰)
        - Reward = æ—¥å ±é…¬ç‡ (é¼“å‹µå¿«é€Ÿè³ºéŒ¢)
        
        ğŸ’¡ **è¨“ç·´ç›®æ¨™**:
        - ä¸æ˜¯ã€Œè³ºéŒ¢ã€
        - è€Œæ˜¯ã€Œå¿«é€Ÿè³ºéŒ¢ã€
        - Agent å­¸æœƒæ¿€é€²åŠ å€‰
        
        âš ï¸ **æ¥µé«˜é¢¨éšª**:
        - å¯èƒ½çˆ†å€‰ (-100%)
        - å¯èƒ½éæ“¬åˆ
        - é»‘ç›’ï¼Œä¸å¯é æ¸¬
        - ä½†æœ‰æœ€é«˜æ½›åŠ› (+150%+)
        """)

    if not SB3_AVAILABLE:
        st.error("éœ€è¦å®‰è£ stable-baselines3: pip install stable-baselines3")
        return

    st.markdown("---")
    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**æ•¸æ“šè¨­å®š**")
        symbol_list = symbol_selector("strategy_k", multi=False)
        symbol = symbol_list[0]
        train_steps = st.slider("è¨“ç·´æ­¥æ•¸", 10000, 100000, 50000, 10000, key="train_k")

    with col2:
        st.markdown("**å›ºå®šåƒæ•¸**")
        st.metric("è³‡é‡‘", "$10,000")
        st.metric("æ§“æ¡¿", "10x ğŸ”¥")
        st.metric("æœ€å¤§å€‰ä½", "200%")
        st.metric("Reward", "æ—¥å ±é…¬ç‡")

    if st.button("ğŸ¤– è¨“ç·´æ¿€é€² Agent", type="primary", use_container_width=True):
        prog = st.progress(0)
        stat = st.empty()
        
        try:
            stat.text("è¼‰å…¥æ•¸æ“š...")
            prog.progress(10)
            
            mtf_loader = MultiTimeframeLoader(loader)
            df_15m, df_1h, df_1d = mtf_loader.load_multi_timeframe(symbol, 120)
            
            # æº–å‚™è¨“ç·´æ•¸æ“š
            df_train = df_1h.iloc[:int(len(df_1h)*0.75)].copy()
            df_test = df_1h.iloc[int(len(df_1h)*0.75):].copy()
            
            stat.text("å»ºç«‹æ¿€é€²ç’°å¢ƒ...")
            prog.progress(20)
            
            env = DummyVecEnv([lambda: AggressiveTradingEnv(df_train, leverage=10)])
            
            stat.text(f"è¨“ç·´ Agent ({train_steps} steps)...")
            prog.progress(30)
            
            model = PPO(
                'MlpPolicy',
                env,
                learning_rate=0.0005,
                n_steps=2048,
                batch_size=64,
                n_epochs=10,
                gamma=0.95,  # æ›´é‡è¦–è¿‘æœŸ reward
                verbose=0
            )
            
            # è¨“ç·´
            for i in range(0, train_steps, train_steps//5):
                model.learn(total_timesteps=train_steps//5, reset_num_timesteps=False)
                prog.progress(30 + int(50 * (i+train_steps//5) / train_steps))
            
            stat.text("æ¸¬è©¦ Agent...")
            prog.progress(85)
            
            # æ¸¬è©¦
            test_env = AggressiveTradingEnv(df_test, leverage=10)
            obs, _ = test_env.reset()
            
            while True:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = test_env.step(action)
                if done or truncated:
                    break
            
            prog.progress(100)
            stat.text("å®Œæˆ")
            
            # çµæœ
            final_balance = test_env.balance
            total_return = (final_balance - 10000) / 10000 * 100
            trades = test_env.trades
            
            st.markdown("### Agent è¡¨ç¾")
            c1, c2, c3 = st.columns(3)
            c1.metric("æœ€çµ‚æ¬Šç›Š", f"${final_balance:,.0f}", f"{final_balance - 10000:+,.0f}")
            c2.metric("ç¸½å ±é…¬", f"{total_return:.1f}%",
                     "ğŸ‰" if total_return >= 100 else ("ğŸ”¥" if total_return >= 50 else "ğŸ“ˆ"))
            c3.metric("äº¤æ˜“æ¬¡æ•¸", len(trades))
            
            if len(trades) > 0:
                wins = [t for t in trades if t['pnl'] > 0]
                losses = [t for t in trades if t['pnl'] <= 0]
                win_rate = len(wins) / len(trades) * 100
                avg_win = np.mean([t['pnl'] for t in wins]) if wins else 0
                avg_loss = np.mean([t['pnl'] for t in losses]) if losses else 0
                
                c1, c2, c3 = st.columns(3)
                c1.metric("å‹ç‡", f"{win_rate:.1f}%")
                c2.metric("å¹³å‡ç²åˆ©", f"${avg_win:.2f}")
                c3.metric("å¹³å‡è™§æ", f"${avg_loss:.2f}")
            
            # è©•åˆ†
            if total_return >= 150:
                st.success("ğŸš€ ç¥ç´šè¡¨ç¾! è¶…é 150%!")
            elif total_return >= 100:
                st.success("ğŸ‰ å®Œç¾é”æ¨™! +100%!")
            elif total_return >= 50:
                st.info("ğŸ”¥ æ¥è¿‘ç›®æ¨™! +50%+")
            elif total_return > 0:
                st.warning("ğŸ“ˆ æœ‰ç›ˆåˆ©ä½†æœªé”æ¨™")
            elif total_return > -50:
                st.warning("âš ï¸ å°å¹…è™§æ")
            else:
                st.error("ğŸ’¥ å¤§å¹…è™§æ/çˆ†å€‰")
            
            # äº¤æ˜“è¨˜éŒ„
            if trades:
                st.subheader("Agent äº¤æ˜“è¨˜éŒ„")
                trades_df = pd.DataFrame(trades)
                st.dataframe(trades_df.tail(20), use_container_width=True)
        
        except Exception as e:
            st.error(f"éŒ¯èª¤: {e}")
            import traceback
            with st.expander("è©³æƒ…"): st.code(traceback.format_exc())
