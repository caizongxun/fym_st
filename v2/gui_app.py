import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from datetime import datetime, timedelta
import os
import sys
import glob
import time

from data_loader import CryptoDataLoader
from feature_engineering import FeatureEngineer
from label_generation import LabelGenerator
from pipeline import TradingPipeline
from model_trainer import ModelTrainer, TrendFilterTrainer
from inference_engine import InferenceEngine
from advanced_data_collector import BatchAdvancedDataCollector, BinanceAdvancedDataCollector
from advanced_feature_merger import AdvancedFeatureMerger


st.set_page_config(
    page_title="V2 äº¤æ˜“ç³»çµ±",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("V2 æ¨¡å¡ŠåŒ–äº¤æ˜“ç³»çµ±")

if 'data_loader' not in st.session_state:
    st.session_state.data_loader = CryptoDataLoader()

if 'pipeline' not in st.session_state:
    st.session_state.pipeline = TradingPipeline()

if 'batch_collector' not in st.session_state:
    st.session_state.batch_collector = BatchAdvancedDataCollector()

if 'feature_merger' not in st.session_state:
    st.session_state.feature_merger = AdvancedFeatureMerger()

tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "ğŸ“Š [1] æ•¸æ“šè¼‰å…¥",
    "ğŸ”„ [2] é€²éšæ•¸æ“šæ”¶é›†",
    "ğŸ› ï¸ [3] ç‰¹å¾µå·¥ç¨‹",
    "ğŸ·ï¸ [4] æ¨™ç±¤ç”Ÿæˆ",
    "ğŸ¤– [5] æ¨¡å‹è¨“ç·´",
    "ğŸ¯ [6] æ¨è«–æ¸¬è©¦",
    "â˜ï¸ [7] HF ä¸Šå‚³"
])

with tab1:
    st.header("æ•¸æ“šè¼‰å…¥")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("è³‡æ–™é›†è³‡è¨Š")
        info = st.session_state.data_loader.get_dataset_info()
        st.metric("äº¤æ˜“å°æ•¸é‡", info['total_symbols'])
        st.metric("æ™‚é–“æ¡†æ¶", len(info['timeframes']))
        st.metric("ç¸½æª”æ¡ˆæ•¸", info['total_files'])
        
        with st.expander("æŸ¥çœ‹æ‰€æœ‰äº¤æ˜“å°"):
            for symbol in info['symbols']:
                st.text(symbol)
    
    with col2:
        st.subheader("è¼‰å…¥æ•¸æ“š")
        
        col2_1, col2_2, col2_3 = st.columns(3)
        
        with col2_1:
            symbol = st.selectbox(
                "é¸æ“‡äº¤æ˜“å°",
                info['symbols'],
                key='load_symbol'
            )
        
        with col2_2:
            timeframe = st.selectbox(
                "é¸æ“‡æ™‚é–“æ¡†æ¶",
                info['timeframes'],
                key='load_timeframe'
            )
        
        with col2_3:
            st.write("")
            st.write("")
            if st.button("è¼‰å…¥æ•¸æ“š", use_container_width=True):
                with st.spinner('è¼‰å…¥ä¸­...'):
                    try:
                        df = st.session_state.data_loader.load_klines(symbol, timeframe)
                        df_prepared = st.session_state.data_loader.prepare_dataframe(df)
                        st.session_state.df_raw = df_prepared
                        st.session_state.current_symbol = symbol
                        st.session_state.current_timeframe = timeframe
                        st.success(f"æˆåŠŸè¼‰å…¥ {len(df_prepared)} ç­†æ•¸æ“š")
                    except Exception as e:
                        st.error(f"è¼‰å…¥å¤±æ•—: {str(e)}")
        
        if 'df_raw' in st.session_state:
            st.subheader("æ•¸æ“šé è¦½")
            
            df_display = st.session_state.df_raw.copy()
            st.dataframe(df_display.head(100), use_container_width=True, height=300)
            
            st.subheader("æ•¸æ“šçµ±è¨ˆ")
            col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
            with col_stat1:
                st.metric("ç¸½ç­†æ•¸", len(df_display))
            with col_stat2:
                st.metric("èµ·å§‹æ™‚é–“", df_display['timestamp'].min().strftime('%Y-%m-%d'))
            with col_stat3:
                st.metric("çµæŸæ™‚é–“", df_display['timestamp'].max().strftime('%Y-%m-%d'))
            with col_stat4:
                st.metric("å¹³å‡åƒ¹æ ¼", f"{df_display['close'].mean():.2f}")

with tab2:
    st.header("ğŸ”„ é€²éšæ•¸æ“šæ”¶é›†")
    
    st.info("ğŸ“ˆ è‡ªå‹•æ”¶é›†æ‰€æœ‰å¯ç”¨æ­·å²æ•¸æ“š (è¨‚å–®æµ/CVD/è³‡é‡‘è²»ç‡/æœªå¹³å€‰é‡/å¤šç©ºæ¯”),é æœŸæå‡æ¸¬è©¦ AUC 0.15-0.25")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("æ”¶é›†åƒæ•¸")
        
        collection_mode = st.radio(
            "æ”¶é›†æ¨¡å¼",
            ["å–®ä¸€å¹£ç¨®", "æ‰¹é‡æ”¶é›† (38å€‹)"],
            key='collection_mode'
        )
        
        if collection_mode == "å–®ä¸€å¹£ç¨®":
            symbols_to_collect = [st.selectbox(
                "é¸æ“‡å¹£ç¨®",
                st.session_state.batch_collector.hf_symbols,
                key='adv_symbol'
            )]
        else:
            symbols_to_collect = st.session_state.batch_collector.hf_symbols
            with st.expander(f"æŸ¥çœ‹å¹£ç¨®æ¸…å–® ({len(symbols_to_collect)}å€‹)"):
                for sym in symbols_to_collect:
                    st.text(sym)
        
        st.write("---")
        
        st.info("""
        ğŸ” **è‡ªå‹•æ¨¡å¼**
        - è‡ªå‹•åµæ¸¬æ¯å€‹å¹£ç¨®æœ€æ—©å¯ç”¨æ™‚é–“
        - å¾€å‰çˆ¬å–ç›´åˆ°ç„¡æ•¸æ“šå¯çˆ¬
        - è‡ªå‹•å„²å­˜è‡³æŒ‡å®šç›®éŒ„
        """)
        
        adv_timeframe = st.selectbox(
            "æ™‚é–“æ¡†æ¶",
            ['15m', '1h', '4h'],
            index=0,
            key='adv_timeframe'
        )
        
        output_dir = st.text_input(
            "è¼¸å‡ºç›®éŒ„",
            value='v2/advanced_data',
            key='adv_output_dir'
        )
        
        st.write("---")
        
        if st.button("ğŸš€ é–‹å§‹æ”¶é›†å…¨éƒ¨æ­·å²æ•¸æ“š", use_container_width=True, type="primary"):
            st.session_state.collection_started = True
            st.rerun()
    
    with col2:
        if st.session_state.get('collection_started', False):
            st.subheader("æ”¶é›†é€²åº¦")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            result_placeholder = st.empty()
            
            collector = BinanceAdvancedDataCollector()
            summary_data = []
            
            for idx, symbol in enumerate(symbols_to_collect):
                progress = (idx + 1) / len(symbols_to_collect)
                progress_bar.progress(progress)
                status_text.text(f"æ­£åœ¨è™•ç† {symbol} ({idx+1}/{len(symbols_to_collect)})...")
                
                try:
                    features_dict = collector.collect_all_advanced_features(
                        symbol=symbol,
                        start_date=None,
                        end_date=None,
                        timeframe=adv_timeframe
                    )
                    
                    collector.save_advanced_features(
                        symbol=symbol,
                        features_dict=features_dict,
                        output_dir=output_dir
                    )
                    
                    summary_data.append({
                        'å¹£ç¨®': symbol,
                        'è¨‚å–®æµ': len(features_dict.get('order_flow', pd.DataFrame())),
                        'è³‡é‡‘è²»ç‡': len(features_dict.get('funding_rate', pd.DataFrame())),
                        'æœªå¹³å€‰é‡': len(features_dict.get('open_interest', pd.DataFrame())),
                        'å¤šç©ºæ¯”': len(features_dict.get('long_short_ratio', pd.DataFrame())),
                        'ä¸»å‹•è²·è³£': len(features_dict.get('taker_buy_sell', pd.DataFrame())),
                        'ç‹€æ…‹': 'âœ… æˆåŠŸ'
                    })
                    
                    time.sleep(1)
                    
                except Exception as e:
                    summary_data.append({
                        'å¹£ç¨®': symbol,
                        'è¨‚å–®æµ': 0,
                        'è³‡é‡‘è²»ç‡': 0,
                        'æœªå¹³å€‰é‡': 0,
                        'å¤šç©ºæ¯”': 0,
                        'ä¸»å‹•è²·è³£': 0,
                        'ç‹€æ…‹': f'âŒ {str(e)[:20]}'
                    })
            
            progress_bar.progress(1.0)
            status_text.text("âœ… æ”¶é›†å®Œæˆ!")
            
            st.session_state.collection_summary = pd.DataFrame(summary_data)
            st.session_state.collection_started = False
            
            summary_path = os.path.join(output_dir, 'collection_summary.csv')
            os.makedirs(output_dir, exist_ok=True)
            st.session_state.collection_summary.to_csv(summary_path, index=False)
            
            st.success(f"âœ… å·²å®Œæˆ {len(symbols_to_collect)} å€‹å¹£ç¨®çš„æ•¸æ“šæ”¶é›†")
            st.rerun()
        
        if 'collection_summary' in st.session_state:
            st.subheader("ğŸ“‹ æ”¶é›†æ‘˜è¦")
            st.dataframe(
                st.session_state.collection_summary,
                use_container_width=True,
                hide_index=True,
                height=400
            )
            
            success_count = (st.session_state.collection_summary['ç‹€æ…‹'] == 'âœ… æˆåŠŸ').sum()
            total_records = st.session_state.collection_summary[['è¨‚å–®æµ', 'è³‡é‡‘è²»ç‡', 'æœªå¹³å€‰é‡', 'å¤šç©ºæ¯”', 'ä¸»å‹•è²·è³£']].sum().sum()
            
            col_m1, col_m2, col_m3 = st.columns(3)
            with col_m1:
                st.metric("æˆåŠŸæ”¶é›†", f"{success_count}/{len(st.session_state.collection_summary)}")
            with col_m2:
                st.metric("ç¸½æ•¸æ“šç­†æ•¸", f"{total_records:,}")
            with col_m3:
                success_rate = (success_count / len(st.session_state.collection_summary) * 100)
                st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")

with tab3:
    st.header("ç‰¹å¾µå·¥ç¨‹")
    
    if 'df_raw' not in st.session_state:
        st.warning("âš ï¸ è«‹å…ˆåœ¨ [1]æ•¸æ“šè¼‰å…¥ é é¢è¼‰å…¥æ•¸æ“š")
    else:
        col1, col2 = st.columns([1, 3])
        
        with col1:
            st.subheader("åƒæ•¸è¨­å®š")
            
            bb_period = st.number_input("å¸ƒæ—å¸¶é€±æœŸ", 5, 50, 20)
            bb_std = st.number_input("æ¨™æº–å·®å€æ•¸", 1.0, 3.0, 2.0, 0.1)
            lookback = st.number_input("å›æº¯é€±æœŸ", 50, 200, 100)
            pivot_left = st.number_input("æ¨ç´å·¦å´Kç·š", 1, 10, 3)
            pivot_right = st.number_input("æ¨ç´å³å´Kç·š", 1, 10, 3)
            
            st.write("---")
            
            merge_advanced = st.checkbox(
                "ğŸ”¥ åˆä½µé€²éšç‰¹å¾µ",
                value=False,
                help="è‡ªå‹•è¼‰å…¥è¨‚å–®æµã€è³‡é‡‘è²»ç‡ç­‰é€²éšæ•¸æ“š"
            )
            
            if st.button("è¨ˆç®—ç‰¹å¾µ", use_container_width=True):
                with st.spinner('è¨ˆç®—ä¸­...'):
                    try:
                        fe = FeatureEngineer(
                            bb_period=bb_period,
                            bb_std=bb_std,
                            lookback=lookback,
                            pivot_left=pivot_left,
                            pivot_right=pivot_right
                        )
                        st.session_state.df_features = fe.process_features(st.session_state.df_raw)
                        st.session_state.feature_engineer = fe
                        
                        base_features = len(fe.get_feature_columns())
                        
                        if merge_advanced:
                            if 'current_symbol' in st.session_state:
                                merger = st.session_state.feature_merger
                                st.session_state.df_features = merger.merge_all_features(
                                    st.session_state.df_features,
                                    st.session_state.current_symbol
                                )
                                adv_features = merger.get_advanced_feature_columns(st.session_state.df_features)
                                st.success(f"âœ… ç‰¹å¾µè¨ˆç®—å®Œæˆ: {len(st.session_state.df_features)} ç­† | åŸºç¤ç‰¹å¾µ: {base_features} | é€²éšç‰¹å¾µ: {len(adv_features)}")
                            else:
                                st.warning("âš ï¸ ç„¡æ³•åˆä½µé€²éšç‰¹å¾µ: è«‹å…ˆè¼‰å…¥æ•¸æ“š")
                                st.success(f"âœ… åŸºç¤ç‰¹å¾µè¨ˆç®—å®Œæˆ: {len(st.session_state.df_features)} ç­† | ç‰¹å¾µæ•¸: {base_features}")
                        else:
                            st.success(f"âœ… åŸºç¤ç‰¹å¾µè¨ˆç®—å®Œæˆ: {len(st.session_state.df_features)} ç­† | ç‰¹å¾µæ•¸: {base_features}")
                    except Exception as e:
                        st.error(f"âŒ è¨ˆç®—å¤±æ•—: {str(e)}")
        
        with col2:
            if 'df_features' in st.session_state:
                st.subheader("ç‰¹å¾µæ•¸æ“šé è¦½")
                
                feature_cols = st.session_state.feature_engineer.get_feature_columns()
                
                adv_features = []
                if 'feature_merger' in st.session_state:
                    adv_features = st.session_state.feature_merger.get_advanced_feature_columns(
                        st.session_state.df_features
                    )
                
                all_features = feature_cols + adv_features
                display_cols = ['timestamp', 'close'] + all_features[:8]
                available_cols = [c for c in display_cols if c in st.session_state.df_features.columns]
                
                st.dataframe(
                    st.session_state.df_features[available_cols].head(50),
                    use_container_width=True,
                    height=300
                )
                
                st.subheader("ğŸ“Š ç‰¹å¾µåˆ—è¡¨")
                
                col_feat1, col_feat2 = st.columns(2)
                with col_feat1:
                    st.write("**åŸºç¤ç‰¹å¾µ**")
                    for feat in feature_cols:
                        st.text(f"â€¢ {feat}")
                
                with col_feat2:
                    if adv_features:
                        st.write(f"**é€²éšç‰¹å¾µ ({len(adv_features)}å€‹)**")
                        for feat in adv_features[:15]:
                            st.text(f"â€¢ {feat}")
                        if len(adv_features) > 15:
                            st.text(f"... å’Œå…¶ä»– {len(adv_features)-15} å€‹")
                    else:
                        st.info("æœªè¼‰å…¥é€²éšç‰¹å¾µ")

with tab4:
    st.header("æ¨™ç±¤ç”Ÿæˆ")
    
    if 'df_features' not in st.session_state:
        st.warning("âš ï¸ è«‹å…ˆåœ¨ [3]ç‰¹å¾µå·¥ç¨‹ é é¢è¨ˆç®—ç‰¹å¾µ")
    else:
        col1, col2 = st.columns([1, 3])
        
        with col1:
            st.subheader("åƒæ•¸è¨­å®š")
            
            atr_period = st.number_input("ATRé€±æœŸ", 5, 30, 14)
            sl_mult = st.number_input("åœæATRå€æ•¸", 0.5, 3.0, 1.5, 0.1)
            tp_mult = st.number_input("åœåˆ©ATRå€æ•¸", 1.0, 5.0, 3.0, 0.1)
            lookahead = st.number_input("å‰ç¥Kç·šæ•¸", 5, 50, 16)
            
            if st.button("ç”Ÿæˆæ¨™ç±¤", use_container_width=True):
                with st.spinner('ç”Ÿæˆä¸­...'):
                    try:
                        lg = LabelGenerator(
                            atr_period=atr_period,
                            sl_atr_mult=sl_mult,
                            tp_atr_mult=tp_mult,
                            lookahead_bars=lookahead
                        )
                        st.session_state.df_labeled = lg.generate_labels(st.session_state.df_features)
                        st.session_state.label_generator = lg
                        
                        stats = lg.get_label_statistics(st.session_state.df_labeled)
                        st.session_state.label_stats = stats
                        
                        st.success("âœ… æ¨™ç±¤ç”Ÿæˆå®Œæˆ")
                    except Exception as e:
                        st.error(f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}")
        
        with col2:
            if 'df_labeled' in st.session_state:
                st.subheader("æ¨™ç±¤çµ±è¨ˆ")
                
                stats = st.session_state.label_stats
                
                col_stat1, col_stat2 = st.columns(2)
                
                with col_stat1:
                    st.write("**åšå¤šæ¨£æœ¬**")
                    if 'long_total' in stats:
                        st.metric("ç¸½æ•¸", stats['long_total'])
                        st.metric("æˆåŠŸ", stats['long_success'])
                        st.metric("å¤±æ•—", stats['long_fail'])
                        st.metric("æˆåŠŸç‡", f"{stats['long_success_rate']:.2f}%")
                    else:
                        st.info("ç„¡åšå¤šæ¨£æœ¬")
                
                with col_stat2:
                    st.write("**åšç©ºæ¨£æœ¬**")
                    if 'short_total' in stats:
                        st.metric("ç¸½æ•¸", stats['short_total'])
                        st.metric("æˆåŠŸ", stats['short_success'])
                        st.metric("å¤±æ•—", stats['short_fail'])
                        st.metric("æˆåŠŸç‡", f"{stats['short_success_rate']:.2f}%")
                    else:
                        st.info("ç„¡åšç©ºæ¨£æœ¬")
                
                st.subheader("æ¨™ç±¤æ•¸æ“šé è¦½")
                display_cols = ['timestamp', 'close', 'lower', 'upper', 'atr', 
                               'is_touching_lower', 'is_touching_upper', 
                               'target_long', 'target_short']
                available_cols = [col for col in display_cols if col in st.session_state.df_labeled.columns]
                st.dataframe(
                    st.session_state.df_labeled[available_cols].head(50),
                    use_container_width=True,
                    height=300
                )

with tab5:
    st.header("æ¨¡å‹è¨“ç·´")
    
    if 'df_labeled' not in st.session_state:
        st.warning("âš ï¸ è«‹å…ˆåœ¨ [4]æ¨™ç±¤ç”Ÿæˆ é é¢ç”Ÿæˆæ¨™ç±¤")
    else:
        col1, col2 = st.columns([1, 3])
        
        with col1:
            st.subheader("è¨“ç·´åƒæ•¸")
            
            direction = st.selectbox("æ–¹å‘", ['long', 'short'])
            n_estimators = st.number_input("æ¨¹æ•¸é‡", 100, 1000, 300, 50)
            learning_rate = st.number_input("å­¸ç¿’ç‡", 0.001, 0.1, 0.01, 0.001, format="%.3f")
            max_depth = st.number_input("æœ€å¤§æ·±åº¦", 3, 15, 4)
            train_ratio = st.number_input("è¨“ç·´é›†æ¯”ä¾‹", 0.5, 0.9, 0.8, 0.05)
            
            st.write("---")
            
            col_btn1, col_btn2 = st.columns(2)
            
            with col_btn1:
                if st.button("è¨“ç·´åå½ˆæ¨¡å‹", use_container_width=True):
                    with st.spinner('è¨“ç·´ä¸­...'):
                        try:
                            df_train = st.session_state.label_generator.prepare_training_data(
                                st.session_state.df_labeled,
                                direction=direction
                            )
                            
                            trainer = ModelTrainer(
                                model_type='bounce',
                                n_estimators=n_estimators,
                                learning_rate=learning_rate,
                                max_depth=max_depth
                            )
                            
                            results = trainer.train(df_train, train_ratio=train_ratio)
                            
                            os.makedirs('v2/models', exist_ok=True)
                            trainer.save_model(f'v2/models/bounce_{direction}_model.pkl')
                            
                            st.session_state.bounce_results = results
                            st.success("âœ… åå½ˆæ¨¡å‹è¨“ç·´å®Œæˆ")
                        except Exception as e:
                            st.error(f"âŒ è¨“ç·´å¤±æ•—: {str(e)}")
            
            with col_btn2:
                if st.button("è¨“ç·´éæ¿¾æ¨¡å‹", use_container_width=True):
                    with st.spinner('è¨“ç·´ä¸­...'):
                        try:
                            df_train = st.session_state.label_generator.prepare_training_data(
                                st.session_state.df_labeled,
                                direction=direction
                            )
                            
                            trainer = TrendFilterTrainer(
                                n_estimators=n_estimators,
                                learning_rate=learning_rate,
                                max_depth=max_depth
                            )
                            
                            results = trainer.train(df_train, train_ratio=train_ratio)
                            
                            os.makedirs('v2/models', exist_ok=True)
                            trainer.save_model(f'v2/models/filter_{direction}_model.pkl')
                            
                            st.session_state.filter_results = results
                            st.success("âœ… éæ¿¾æ¨¡å‹è¨“ç·´å®Œæˆ")
                        except Exception as e:
                            st.error(f"âŒ è¨“ç·´å¤±æ•—: {str(e)}")
        
        with col2:
            st.subheader("è¨“ç·´çµæœ")
            
            col_res1, col_res2 = st.columns(2)
            
            with col_res1:
                st.write("**åå½ˆæ¨¡å‹**")
                if 'bounce_results' in st.session_state:
                    results = st.session_state.bounce_results
                    st.metric("è¨“ç·´ AUC", f"{results['train_auc']:.4f}")
                    st.metric("æ¸¬è©¦ AUC", f"{results['test_auc']:.4f}")
                    st.metric("è¨“ç·´æ¨£æœ¬", results['train_samples'])
                    st.metric("æ¸¬è©¦æ¨£æœ¬", results['test_samples'])
                    
                    st.write("**ç‰¹å¾µé‡è¦æ€§ Top 10**")
                    st.dataframe(
                        results['feature_importance'].head(10),
                        use_container_width=True,
                        hide_index=True,
                        height=300
                    )
                else:
                    st.info("å°šæœªè¨“ç·´")
            
            with col_res2:
                st.write("**éæ¿¾æ¨¡å‹**")
                if 'filter_results' in st.session_state:
                    results = st.session_state.filter_results
                    st.metric("è¨“ç·´ AUC", f"{results['train_auc']:.4f}")
                    st.metric("æ¸¬è©¦ AUC", f"{results['test_auc']:.4f}")
                    st.metric("è¨“ç·´æ¨£æœ¬", results['train_samples'])
                    st.metric("æ¸¬è©¦æ¨£æœ¬", results['test_samples'])
                    
                    st.write("**ç‰¹å¾µé‡è¦æ€§ Top 10**")
                    st.dataframe(
                        results['feature_importance'].head(10),
                        use_container_width=True,
                        hide_index=True,
                        height=300
                    )
                else:
                    st.info("å°šæœªè¨“ç·´")

with tab6:
    st.header("æ¨è«–æ¸¬è©¦")
    
    col1, col2 = st.columns([1, 3])
    
    with col1:
        st.subheader("æ¨¡å‹é¸æ“‡")
        
        direction_infer = st.selectbox("æ–¹å‘", ['long', 'short'], key='infer_direction')
        
        bounce_path = f'v2/models/bounce_{direction_infer}_model.pkl'
        filter_path = f'v2/models/filter_{direction_infer}_model.pkl'
        
        if os.path.exists(bounce_path) and os.path.exists(filter_path):
            st.success("âœ… æ¨¡å‹æª”æ¡ˆå­˜åœ¨")
            
            st.subheader("é–¾å€¼è¨­å®š")
            bounce_threshold = st.slider("åå½ˆé–¾å€¼", 0.0, 1.0, 0.65, 0.05)
            filter_threshold = st.slider("éæ¿¾é–¾å€¼", 0.0, 1.0, 0.40, 0.05)
            
            if st.button("åŸ·è¡Œæ¨è«–", use_container_width=True):
                if 'df_labeled' not in st.session_state:
                    st.error("âŒ è«‹å…ˆç”Ÿæˆæ¨™ç±¤æ•¸æ“š")
                else:
                    with st.spinner('æ¨è«–ä¸­...'):
                        try:
                            engine = InferenceEngine(
                                bounce_model_path=bounce_path,
                                filter_model_path=filter_path,
                                bounce_threshold=bounce_threshold,
                                filter_threshold=filter_threshold
                            )
                            
                            df_test = st.session_state.label_generator.prepare_training_data(
                                st.session_state.df_labeled,
                                direction=direction_infer
                            )
                            
                            df_predictions = engine.predict_batch(df_test)
                            stats = engine.get_statistics(df_predictions)
                            
                            st.session_state.df_predictions = df_predictions
                            st.session_state.inference_stats = stats
                            
                            st.success("âœ… æ¨è«–å®Œæˆ")
                        except Exception as e:
                            st.error(f"âŒ æ¨è«–å¤±æ•—: {str(e)}")
        else:
            st.error(f"âŒ æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨")
            st.info("è«‹å…ˆåœ¨ [5]æ¨¡å‹è¨“ç·´ é é¢è¨“ç·´æ¨¡å‹")
    
    with col2:
        if 'inference_stats' in st.session_state:
            st.subheader("æ¨è«–çµ±è¨ˆ")
            
            stats = st.session_state.inference_stats
            
            col_stat1, col_stat2, col_stat3, col_stat4 = st.columns(4)
            
            with col_stat1:
                st.metric("ç¸½æ¨£æœ¬", stats['total_samples'])
            with col_stat2:
                st.metric("æ ¸å‡†é€²å ´", stats['entry_approved'])
            with col_stat3:
                st.metric("é€²å ´ç‡", f"{stats['entry_rate']:.2f}%")
            with col_stat4:
                if 'approved_success_rate' in stats:
                    st.metric("æ ¸å‡†å¾ŒæˆåŠŸç‡", f"{stats['approved_success_rate']:.2f}%")
            
            col_stat5, col_stat6 = st.columns(2)
            with col_stat5:
                st.metric("å¹³å‡ P_bounce", f"{stats['avg_p_bounce']:.4f}")
            with col_stat6:
                st.metric("å¹³å‡ P_filter", f"{stats['avg_p_filter']:.4f}")
            
            st.subheader("è¨Šè™ŸåŸå› åˆ†ä½ˆ")
            reason_df = pd.DataFrame([
                {'reason': k, 'count': v} 
                for k, v in stats['reason_counts'].items()
            ])
            st.dataframe(reason_df, use_container_width=True, hide_index=True)
            
            st.subheader("æ¨è«–çµæœé è¦½")
            display_cols = ['timestamp', 'close', 'p_bounce', 'p_filter', 
                           'signal', 'reason', 'target']
            available_cols = [col for col in display_cols if col in st.session_state.df_predictions.columns]
            st.dataframe(
                st.session_state.df_predictions[available_cols].head(50),
                use_container_width=True,
                height=300
            )
        else:
            st.info("è«‹å…ˆåŸ·è¡Œæ¨è«–")

with tab7:
    st.header("â˜ï¸ HuggingFace ä¸Šå‚³")
    
    st.info("ğŸ“¤ å°‡é€²éšæ•¸æ“šä¸Šå‚³åˆ° HuggingFace ä»¥ä¾¿åˆ†äº«èˆ‡å‚™ä»½")
    
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.subheader("ä¸Šå‚³è¨­å®š")
        
        hf_token = st.text_input(
            "HuggingFace Token",
            type="password",
            help="å¾ https://huggingface.co/settings/tokens ç²å–",
            key='hf_token'
        )
        
        hf_repo = st.text_input(
            "Repository åç¨±",
            value="username/v2-crypto-advanced-data",
            help="æ ¼å¼: username/repo-name",
            key='hf_repo'
        )
        
        data_dir = st.text_input(
            "æ•¸æ“šç›®éŒ„",
            value="v2/advanced_data",
            key='hf_data_dir'
        )
        
        st.write("---")
        
        if st.button("ğŸš€ ä¸Šå‚³åˆ° HuggingFace", use_container_width=True, type="primary"):
            if not hf_token:
                st.error("âŒ è«‹è¼¸å…¥ HuggingFace Token")
            elif not os.path.exists(data_dir):
                st.error(f"âŒ æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {data_dir}")
            else:
                with st.spinner('ä¸Šå‚³ä¸­...'):
                    try:
                        from huggingface_hub import HfApi, create_repo
                        
                        api = HfApi(token=hf_token)
                        
                        try:
                            create_repo(
                                repo_id=hf_repo,
                                token=hf_token,
                                repo_type="dataset",
                                exist_ok=True
                            )
                            st.success(f"âœ… Repository å·²å»ºç«‹: {hf_repo}")
                        except:
                            st.info(f"â„¹ï¸ Repository å·²å­˜åœ¨: {hf_repo}")
                        
                        parquet_files = glob.glob(os.path.join(data_dir, "*.parquet"))
                        csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
                        all_files = parquet_files + csv_files
                        
                        if not all_files:
                            st.error(f"âŒ åœ¨ {data_dir} ä¸­æ‰¾ä¸åˆ°ä»»ä½•æª”æ¡ˆ")
                        else:
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            uploaded_files = []
                            
                            for idx, file_path in enumerate(all_files):
                                progress = (idx + 1) / len(all_files)
                                progress_bar.progress(progress)
                                
                                filename = os.path.basename(file_path)
                                status_text.text(f"ä¸Šå‚³ {filename} ({idx+1}/{len(all_files)})...")
                                
                                try:
                                    api.upload_file(
                                        path_or_fileobj=file_path,
                                        path_in_repo=filename,
                                        repo_id=hf_repo,
                                        repo_type="dataset",
                                        token=hf_token
                                    )
                                    uploaded_files.append(filename)
                                except Exception as e:
                                    st.warning(f"âš ï¸ ä¸Šå‚³ {filename} å¤±æ•—: {str(e)[:30]}")
                            
                            progress_bar.progress(1.0)
                            status_text.text("âœ… ä¸Šå‚³å®Œæˆ!")
                            
                            st.session_state.uploaded_files = uploaded_files
                            st.success(f"âœ… å·²æˆåŠŸä¸Šå‚³ {len(uploaded_files)}/{len(all_files)} å€‹æª”æ¡ˆ")
                            
                    except Exception as e:
                        st.error(f"âŒ ä¸Šå‚³å¤±æ•—: {str(e)}")
    
    with col2:
        st.subheader("ğŸ“ æª¢æŸ¥æ•¸æ“šæª”æ¡ˆ")
        
        data_dir_check = st.text_input(
            "ç›®éŒ„è·¯å¾‘",
            value="v2/advanced_data",
            key='data_dir_check'
        )
        
        if os.path.exists(data_dir_check):
            parquet_files = glob.glob(os.path.join(data_dir_check, "*.parquet"))
            csv_files = glob.glob(os.path.join(data_dir_check, "*.csv"))
            
            col_m1, col_m2 = st.columns(2)
            with col_m1:
                st.metric("Parquet æª”æ¡ˆ", len(parquet_files))
            with col_m2:
                st.metric("CSV æª”æ¡ˆ", len(csv_files))
            
            if parquet_files or csv_files:
                file_info = []
                for f in parquet_files + csv_files:
                    size_mb = os.path.getsize(f) / (1024 * 1024)
                    file_info.append({
                        'æª”æ¡ˆå': os.path.basename(f),
                        'å¤§å° (MB)': f"{size_mb:.2f}",
                        'ä¿®æ”¹æ™‚é–“': datetime.fromtimestamp(os.path.getmtime(f)).strftime('%Y-%m-%d %H:%M')
                    })
                
                df_files = pd.DataFrame(file_info)
                st.dataframe(df_files, use_container_width=True, hide_index=True, height=400)
                
                total_size = sum([os.path.getsize(f) for f in parquet_files + csv_files]) / (1024 * 1024)
                st.metric("ç¸½å¤§å° (MB)", f"{total_size:.2f}")
        else:
            st.warning(f"âš ï¸ ç›®éŒ„ä¸å­˜åœ¨: {data_dir_check}")
        
        if 'uploaded_files' in st.session_state:
            st.subheader("ğŸ“‹ ä¸Šå‚³è¨˜éŒ„")
            st.write(f"å·²ä¸Šå‚³ {len(st.session_state.uploaded_files)} å€‹æª”æ¡ˆ")
            for f in st.session_state.uploaded_files[:10]:
                st.text(f"âœ“ {f}")
            if len(st.session_state.uploaded_files) > 10:
                st.text(f"... å’Œå…¶ä»– {len(st.session_state.uploaded_files)-10} å€‹")

st.sidebar.header("é—œæ–¼")
st.sidebar.info(
    """
    **V2 æ¨¡å¡ŠåŒ–äº¤æ˜“ç³»çµ±**
    
    åŠŸèƒ½æ¨¡å¡Š:
    - ğŸ“Š æ•¸æ“šè¼‰å…¥ (HuggingFace)
    - ğŸ”„ é€²éšæ•¸æ“šæ”¶é›† (Binance API)
    - ğŸ› ï¸ ç‰¹å¾µå·¥ç¨‹ (30+ æŒ‡æ¨™)
    - ğŸ·ï¸ æ¨™ç±¤ç”Ÿæˆ (ATR å‹•æ…‹)
    - ğŸ¤– æ¨¡å‹è¨“ç·´ (LightGBM é˜²éæ“¬åˆ)
    - ğŸ¯ æ¨è«–æ¸¬è©¦ (å…±æŒ¯-å¦æ±º)
    - â˜ï¸ HF ä¸Šå‚³ (å‚™ä»½åˆ†äº«)
    
    ç‰ˆæœ¬: 2.1.0
    """
)

st.sidebar.header("å¿«é€Ÿæ“ä½œ")
if st.sidebar.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç·©å­˜"):
    for key in list(st.session_state.keys()):
        if key not in ['data_loader', 'pipeline', 'batch_collector', 'feature_merger']:
            del st.session_state[key]
    st.sidebar.success("âœ… ç·©å­˜å·²æ¸…é™¤")
    st.rerun()
